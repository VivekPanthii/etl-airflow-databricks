{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c04aac-da10-4743-a983-0b36b1fefde6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c49708bc-05ac-4e8a-a779-67fafb08637b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####  **LOAD BRONZE CRM_INFO TO SILVER CRM_INFO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57b1b4d0-edef-4b99-8bfe-0c67b6d7f8ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the Bronze Delta table as a batch DataFrame (snapshot of current data)\n",
    "\n",
    "df_cust_info=spark.read.format(\"delta\").load(\"/Volumes/workspace/bronze_crm_erp/bronze_volume/bronze_crm_data/cust_info/data/\")\n",
    "\n",
    "# Dictionaries to map abbreviations to full descriptions for marital status and gender\n",
    "\n",
    "marital_status_map={\n",
    "    \"M\":\"Married\",\n",
    "    \"S\":\"Single\"\n",
    "}\n",
    "\n",
    "gender_map={\n",
    "    \"F\":\"Female\",\n",
    "    \"M\":\"Male\"\n",
    "}\n",
    "\n",
    "\n",
    "# Transformation and cleansing logic applied on the streaming DataFrame\n",
    " # Cast customer ID to integer type\n",
    " # Keep cst_key as is (assuming string or correct type already)\n",
    " # Trim leading/trailing spaces in first and last names\n",
    " # Normalize marital status: trim spaces, convert to uppercase, replace abbreviations with full words,\n",
    "    # and fill any nulls with \"Unknown\". Operation is scoped only to the 'cst_marital_status' column.\n",
    " # Normalize gender: trim spaces, convert to uppercase, replace abbreviations with full words,\n",
    "    # and fill any nulls with \"Unknown\". Scoped only to 'cst_gndr' column.\n",
    " # Convert create date column from string to DateType (format: yyyy-MM-dd)\n",
    " # Add current timestamp as a new column indicating when data was processed into the DWH\n",
    " # Drop the '_rescued_data' column generated by Autoloader if it exists (contains malformed rows)\n",
    "silver_df=(\n",
    "    df_cust_info\\\n",
    "        .withColumn(\"cst_id\",col(\"cst_id\").cast(\"int\"))\n",
    "        .withColumn(\"cst_key\",col(\"cst_key\"))\n",
    "        .withColumn(\"cst_firstname\", trim(col(\"cst_firstname\")))\n",
    "        .withColumn(\"cst_lastname\",trim(col(\"cst_lastname\")))\n",
    "        .withColumn(\"cst_marital_status\",trim(upper(col(\"cst_marital_status\")))).na.replace(marital_status_map,subset=[\"cst_marital_status\"]).na.fill({\"cst_marital_status\" : \"Unknown\"},subset=[\"cst_marital_status\"])\n",
    "        .withColumn(\"cst_gndr\",trim(upper(col(\"cst_gndr\")))).na.replace(gender_map,subset=[\"cst_gndr\"]).na.fill({\"cst_gndr\" : \"Unknown\"},subset=[\"cst_gndr\"])\n",
    "        .withColumn(\"cst_create_date\",to_date(col(\"cst_create_date\"),\"yyyy-MM-dd\"))\n",
    "        .withColumn(\"DWH_create_date\", current_timestamp())\n",
    "        .drop(col(\"_rescued_data\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Write transformed data to the Silver Delta table:\n",
    "# - Append new records\n",
    "# - Allow schema evolution if new columns appear\n",
    "silver_df.write\\\n",
    "    .format(\"delta\")\\\n",
    "        .option(\"overwriteSchema\", \"true\")\\\n",
    "            .mode(\"append\")\\\n",
    "                .saveAsTable(\"workspace.silver_crm_erp.cust_info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95d925e4-31fe-43ff-80b4-b71c2c059b15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### **LOAD BRONZE PRD_INFO TO SILVER PRD_INFO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9155900-b0df-4d3f-8abb-9b0c18e66b6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read the Bronze Delta table as a batch DataFrame (snapshot of current data)\n",
    "Split 'prd_key' to extract category components.\n",
    "Define product line mapping.\n",
    "Define window spec for ordering by product start date per product key.\n",
    "\"\"\"\n",
    "\n",
    "df_prd_info=spark.read.format(\"delta\").load(\"/Volumes/workspace/bronze_crm_erp/bronze_volume/bronze_crm_data/prd_info/data/\")\n",
    "\n",
    "split_col= split(col(\"prd_key\"),\"-\")\n",
    "\n",
    "product_map ={\n",
    "    \"R\":\"Road\",\n",
    "    \"S\":\"Other Sales\",\n",
    "    \"M\":\"Mountain\",\n",
    "    \"T\":\"Touring\"\n",
    "}\n",
    "\n",
    "window_spec = Window.partitionBy(col(\"prd_key\")).orderBy(col(\"prd_start_dt\"))\n",
    "\n",
    "\"\"\"\n",
    "Transformations for Silver layer:\n",
    "- Cast 'prd_id' to int type\n",
    "- Create 'cat_id' by concatenating first two parts of 'prd_key' separated by '_'\n",
    "- Rebuild 'prd_key' by concatenating all parts from 3rd onward separated by '-'\n",
    "- Keep product name column as is\n",
    "- Cast 'prd_cost' to int and fill nulls with 0\n",
    "- Normalize 'prd_line' by trimming, uppercasing, replacing codes from mapping, and filling nulls with 'Unknown'\n",
    "- Convert 'prd_start_dt' string to DateType\n",
    "- Compute 'prd_end_dt' using lead window function on 'prd_start_dt' per 'prd_key'\n",
    "- Fill null 'prd_end_dt' values with current date\n",
    "- Add current timestamp column 'DWH_update_date' to mark processing time\n",
    "- Drop '_rescued_data' column if exists (for malformed rows)\n",
    "\"\"\"\n",
    "\n",
    "silver_prd_df=(\n",
    "  df_prd_info\\\n",
    "    .withColumn(\"prd_id\",col(\"prd_id\").cast(\"int\"))\n",
    "    .withColumn(\"cat_id\",concat_ws(\"_\",split_col.getItem(0), split_col.getItem(1)))\\\n",
    "    .withColumn(\"prd_key\", concat_ws('-',slice(split_col,3,size(split_col)-2)))\n",
    "    .withColumn(\"prd_nm\",col(\"prd_nm\"))\n",
    "    .withColumn(\"prd_cost\",col(\"prd_cost\").cast(\"int\")).na.fill({\"prd_cost\":\"0\"}, subset=[\"prd_cost\"])\n",
    "    .withColumn(\"prd_line\",trim(upper(col(\"prd_line\")))).na.replace(product_map,subset=[\"prd_line\"]).na.fill({\"prd_line\":\"Unknown\"}, subset=[\"prd_line\"])\n",
    "    .withColumn(\"prd_start_dt\",to_date(col(\"prd_start_dt\"),\"yyyy-MM-dd\"))\n",
    "    .withColumn(\"prd_end_dt\",lead(col(\"prd_start_dt\"), 1).over(window_spec))\n",
    "    .withColumn('prd_end_dt',coalesce(col(\"prd_end_dt\"),current_date()))\n",
    "    .withColumn(\"DWH_update_date\", current_timestamp())\n",
    "    .drop(col(\"_rescued_data\"))\n",
    "\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Reorder columns so that 'cat_id' appears right after 'prd_id'.\n",
    "\"\"\"\n",
    "def reorder_columns(silver_prd_df,insert_col,after_col):\n",
    "  # Remove 'insert_col' if it already exists to avoid duplication\n",
    "  new_silver_prd_df=[c for c in silver_prd_df.columns if c != insert_col]\n",
    "\n",
    "  # Find index of the column after which to insert\n",
    "  indx=new_silver_prd_df.index(\"prd_id\")\n",
    "\n",
    "  # Insert 'insert_col' right after 'after_col'\n",
    "  new_silver_prd_df=new_silver_prd_df[:indx+1]+[\"cat_id\"]+new_silver_prd_df[indx+1:]\n",
    "\n",
    "  # Reorder DataFrame columns accordingly\n",
    "  silver_prd_df =silver_prd_df.select(new_silver_prd_df)\n",
    "  return silver_prd_df\n",
    "\n",
    "silver_prd_df=reorder_columns(silver_prd_df,\"cat_id\",\"prd_id\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Write transformed data to the Silver Delta table:\n",
    "  - Append new records\n",
    "  - Allow schema evolution if new columns appear\n",
    "\"\"\"\n",
    "\n",
    "silver_prd_df.write\\\n",
    "  .format(\"delta\")\\\n",
    "    .option(\"overwriteSchema\",\"true\")\\\n",
    "      .mode(\"append\")\\\n",
    "        .saveAsTable(\"workspace.silver_crm_erp.prd_info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d37e8445-5aa9-460b-a792-1db0033f7e92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### **LOAD BRONZE SALES_DETAILS TO SILVER SALES_DETAILS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c52a8afa-fd31-4f7b-a16e-0cfc8d8b3596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sales_details=spark.read.format(\"delta\").load(\"/Volumes/workspace/bronze_crm_erp/bronze_volume/bronze_crm_data/sales_details/data/\")\n",
    "\n",
    "\"\"\"\n",
    "Define list of date columns that need transformation from YYYYMMDD format to date type\n",
    "\"\"\"\n",
    "\n",
    "column_lists=[\"sls_order_dt\",\"sls_ship_dt\",\"sls_due_dt\"]\n",
    "\n",
    "\"\"\"\n",
    "Create silver layer transformation pipeline with data quality fixes and type conversions\n",
    "- Convert string fields to appropriate numeric types\n",
    "- Fix inconsistent sales amounts and prices\n",
    "- Add audit timestamps\n",
    "\n",
    "\n",
    "        Select key columns to carry forward, ensuring necessary data types are consistent:\n",
    "        - Keep order number and product key as is since they are identifiers.\n",
    "        - Cast customer ID to float to standardize the data type, possibly because\n",
    "          it may come in different formats or for numeric operations later.\n",
    "        - Cast sales amount to float to prepare for numeric corrections and calculations.\n",
    "\n",
    "        Correct the sales amount where data quality issues exist:\n",
    "        - If sales is NULL or zero/negative, or\n",
    "        - If sales value does not match the expected calculation (quantity * price),\n",
    "          then recalculate sales as quantity multiplied by the absolute price.\n",
    "        This ensures data accuracy, especially if upstream sources had incorrect or missing values.\n",
    "\n",
    "        Clean and correct the sales price with two steps:\n",
    "        1) If price is NULL or invalid (<=0) but quantity is valid (non-zero),\n",
    "           calculate price as sales divided by quantity.\n",
    "        2) If price does not match sales/quantity ratio, override price with recalculated value.\n",
    "        This two-step approach handles different data inconsistencies to ensure price and sales are aligned.\n",
    "\"\"\"\n",
    "silver_sales_df=(\n",
    "    df_sales_details\\\n",
    "        .withColumn(\"sls_ord_num\",col(\"sls_ord_num\"))\n",
    "        .withColumn(\"sls_prd_key\",col(\"sls_prd_key\"))\n",
    "        .withColumn(\"sls_cust_id\",col(\"sls_cust_id\").try_cast(\"int\"))\n",
    "        .withColumn(\"sls_sales\",col(\"sls_sales\").try_cast(\"float\"))\n",
    "        .withColumn(\"sls_sales\",\n",
    "                    when(\n",
    "                        (col(\"sls_sales\").isNull()) |\n",
    "                        (col(\"sls_sales\") <= 0) |\n",
    "                        (col(\"sls_sales\") != col(\"sls_quantity\")*abs(col(\"sls_sales\")/col(\"sls_quantity\"))),\n",
    "                        col(\"sls_quantity\")*abs(col(\"sls_price\"))\n",
    "                    ).otherwise(col(\"sls_sales\"))\n",
    "                )\n",
    "        .withColumn(\"sls_quantity\",col(\"sls_quantity\").try_cast(\"float\"))\n",
    "        .withColumn(\"sls_price\",\n",
    "                    when(\n",
    "                        ((col(\"sls_price\").isNull()) |\n",
    "                            (col(\"sls_price\") <= 0)) & \n",
    "                        (col(\"sls_quantity\") != 0),\n",
    "                        col(\"sls_sales\") / col(\"sls_quantity\")\n",
    "                    ).otherwise(col(\"sls_price\"))\n",
    "                    ).withColumn(\"sls_price\",\n",
    "                    when((col(\"sls_sales\")/col(\"sls_quantity\") != col(\"sls_price\")),\n",
    "                        col(\"sls_sales\")/col(\"sls_quantity\")\n",
    "                    ).otherwise(col(\"sls_price\"))\n",
    "                )\n",
    "        .drop(col(\"_rescued_data\"))\n",
    "        .withColumn(\"DWH_update_date\", current_timestamp())\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Convert each date-related column from string/integer format 'yyyyMMdd' to Spark's date type.\n",
    "The conversion happens only if the string length is exactly 8 and the value is positive,\n",
    "which prevents invalid or malformed data from being converted and avoids runtime errors.\n",
    "If conversion conditions are not met, the column is set to NULL to indicate missing/invalid dates.\n",
    "\"\"\"\n",
    "for column_name in column_lists:\n",
    "    silver_sales_df=silver_sales_df.withColumn(\n",
    "        column_name,\n",
    "            when(\n",
    "                    (length(col(column_name))==8) & (col(column_name) > 0)\n",
    "                    ,to_date(col(column_name),\"yyyyMMdd\")\n",
    "            ).otherwise(lit(None).cast(\"date\"))\n",
    "    )\n",
    "\"\"\"\n",
    "Write the transformed data to silver layer Delta table\n",
    "- Use Delta format for ACID transactions and time travel\n",
    "- Allow schema evolution with overwriteSchema option\n",
    "- Append mode to add new data without replacing existing records\n",
    "\"\"\"\n",
    "silver_sales_df.write\\\n",
    "    .format(\"delta\")\\\n",
    "        .option(\"overwriteSchema\",\"true\")\\\n",
    "            .mode(\"append\")\\\n",
    "                .saveAsTable(\"workspace.silver_crm_erp.sales_details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55bf8f38-88f2-468a-b121-195f6a328f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### **LOAD BRONZE CUST_AZ12 TO SILVER CUST_AZ12**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dff4cc1-9290-4396-8aca-dc40c0e61568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cust_az12=spark.read.format('delta').load(\"/Volumes/workspace/bronze_crm_erp/bronze_volume/bronze_erp_data/CUST_AZ12/data/\")\n",
    "\n",
    "\"\"\"\n",
    "1. Clean the 'CID' column by removing the substring 'NAS' from its values.\n",
    "Then rename the column 'CID' to 'cid' for consistency and lowercase naming convention.\n",
    "\n",
    "2. Convert the 'BDATE' column to date type only if it is a valid date\n",
    "        that is less than or equal to the current date.\n",
    "        Otherwise, set the value to null to avoid invalid or future dates.\n",
    "        Rename 'BDATE' to lowercase 'bdate'.\n",
    "3. Standardize 'GEN' column by trimming spaces and converting to uppercase,\n",
    "        then replace values based on the predefined gender_map dictionary.\n",
    "        Fill any remaining nulls with 'Unknown'.\n",
    "        Finally, rename 'GEN' to lowercase 'gen'.\n",
    "\"\"\"\n",
    "gender_map={\n",
    "    \"M\": \"Male\",\n",
    "    \"F\": \"Female\",\n",
    "    \"FEMALE\":\"Female\",\n",
    "    \"MALE\":\"Male\",\n",
    "    \"\": \"Unknown\"\n",
    "}\n",
    "\n",
    "silver_cust_az12_df=(\n",
    "    df_cust_az12\\\n",
    "        .withColumn(\"CID\",regexp_replace(col(\"CID\"),\"NAS\",\"\"))\n",
    "        .withColumnRenamed(\"CID\",\"cid\")\n",
    "        .withColumn(\"BDATE\",\n",
    "                            when(\n",
    "                                col(\"BDATE\").cast(\"date\") <= current_date(),\n",
    "                                col(\"BDATE\").cast(\"date\")\n",
    "                            ).otherwise(lit(None).cast(\"date\"))\n",
    "                    )\n",
    "        .withColumnRenamed(\"BDATE\",\"bdate\")\n",
    "        .withColumn(\"GEN\",trim(upper(col(\"GEN\")))).na.replace(gender_map,subset=[\"GEN\"]).na.fill(\"Unknown\")\n",
    "        .withColumnRenamed(\"GEN\",\"gen\")\n",
    "        .withColumn(\"DWH_create_date\", current_timestamp())\n",
    "        .drop(\"_rescued_data\")\n",
    " \n",
    ")\n",
    "silver_cust_az12_df.write\\\n",
    "    .format(\"delta\")\\\n",
    "        .mode(\"append\")\\\n",
    "            .option(\"overwriteSchema\",\"true\")\\\n",
    "                .saveAsTable(\"workspace.silver_crm_erp.cust_az12\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "199d0863-c090-4aac-a04d-8d413617b866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### **LOAD BRONZE LOC_A101 TO SILVER LOC_A101**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22a1fc0a-c5d1-4acc-bb50-0420014d690f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_loc_a101= spark.read.format(\"delta\").load(\"/Volumes/workspace/bronze_crm_erp/bronze_volume/bronze_erp_data/LOC_A101/data/\")\n",
    "\n",
    "\"\"\" Transformation and Cleansing\n",
    "\n",
    "        # Step 1: Clean and standardize 'CID' (Customer ID)\n",
    "        # - Remove any \"-\" characters from 'CID' using regexp_replace\n",
    "        # - Trim whitespace around the value\n",
    "        # - Rename the cleaned column from 'CID' to 'cid' (lowercase for consistency)\n",
    "\n",
    "        \n",
    "        # Step 2: Standardize the 'CNTRY' (Country) column\n",
    "        # - Convert country codes to uppercase and trim spaces\n",
    "        # - Map specific values to standardized country names:\n",
    "        #     \"\"    -> \"Unknown\"\n",
    "        #     \"DE\"  -> \"Germany\"\n",
    "        #     \"US\"  -> \"United States\"\n",
    "        #     \"USA\" -> \"United States\"\n",
    "        # - All other values remain unchanged\n",
    "        # - Fill any NULL values with \"Unknown\"\n",
    "        # - Rename column from 'CNTRY' to 'cntry'\n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "silver_loc_a101_df=(\n",
    "    df_loc_a101\\\n",
    "        .withColumn('CID',trim(regexp_replace(col(\"CID\"),\"-\",\"\"))).withColumnRenamed(\"CID\",\"cid\")\n",
    "        .withColumn(\"CNTRY\",\n",
    "                      when(\n",
    "                          trim(upper(col(\"CNTRY\")))==\"\",\"Unknown\"\n",
    "                        ).when(\n",
    "                            trim(upper(col(\"CNTRY\")))==\"DE\",\"Germany\"\n",
    "                        ).when(\n",
    "                            trim(upper(col(\"CNTRY\")))==\"US\",\"United States\"\n",
    "                        ).when(\n",
    "                            trim(upper(col(\"CNTRY\")))==\"USA\",\"United States\"\n",
    "                        ).otherwise(col(\"CNTRY\"))\n",
    "                      ).na.fill(\"Unknown\").withColumnRenamed(\"CNTRY\",\"cntry\")\n",
    "        .withColumn(\"DWH_create_date\", current_timestamp())\n",
    "        .drop(\"_rescued_data\")\n",
    ")\n",
    "\n",
    "silver_loc_a101_df.write\\\n",
    "    .format(\"delta\")\\\n",
    "        .mode(\"append\")\\\n",
    "            .option(\"overwriteSchema\",\"true\")\\\n",
    "                .saveAsTable(\"workspace.silver_crm_erp.loc_a101\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4726c872-55b9-4411-b4a4-863ddb286820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### **LOAD BRONZE PX_CAT_G1V2 TO SILVER PX_CAT_G1V2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c3ed8d5-5aae-4e6d-a8d0-429178e37c94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_px_cat_g1v2=spark.read.format(\"delta\").load(\"/Volumes/workspace/bronze_crm_erp/bronze_volume/bronze_erp_data/PX_CAT_G1V2/data/\")\n",
    "\n",
    "silver_px_cat_g1v2_df=(\n",
    "    df_px_cat_g1v2\\\n",
    "        .withColumnRenamed(\"ID\",\"id\")\n",
    "        .withColumnRenamed(\"CAT_ID\",\"cat_id\")\n",
    "        .withColumnRenamed(\"CAT_NM\",\"cat_nm\")\n",
    "        .withColumnRenamed(\"MAINTENANCE\",\"maintenance\")\n",
    "        .withColumn(\"DWH_update_date\",current_timestamp())\n",
    "        .drop(\"_rescued_data\")\n",
    "\n",
    ")\n",
    "silver_px_cat_g1v2_df.write\\\n",
    "    .format(\"delta\")\\\n",
    "        .mode(\"append\")\\\n",
    "            .option(\"overwriteSchema\",\"true\")\\\n",
    "                .saveAsTable(\"workspace.silver_crm_erp.px_cat_g1v2\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6095872889002248,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
