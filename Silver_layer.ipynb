{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c04aac-da10-4743-a983-0b36b1fefde6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c49708bc-05ac-4e8a-a779-67fafb08637b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####  **LOAD BRONZE CRM_INFO TO SILVER CRM_INFO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57b1b4d0-edef-4b99-8bfe-0c67b6d7f8ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the Bronze Delta table as a batch DataFrame (snapshot of current data)\n",
    "\n",
    "df_cust_info=spark.read.format(\"delta\").load(\"/Volumes/workspace/bronze_crm_erp/bronze_volume/bronze_crm_data/cust_info/data/\")\n",
    "\n",
    "# Dictionaries to map abbreviations to full descriptions for marital status and gender\n",
    "\n",
    "marital_status_map={\n",
    "    \"M\":\"Married\",\n",
    "    \"S\":\"Single\"\n",
    "}\n",
    "\n",
    "gender_map={\n",
    "    \"F\":\"Female\",\n",
    "    \"M\":\"Male\"\n",
    "}\n",
    "\n",
    "\n",
    "# Transformation and cleansing logic applied on the streaming DataFrame\n",
    " # Cast customer ID to integer type\n",
    " # Keep cst_key as is (assuming string or correct type already)\n",
    " # Trim leading/trailing spaces in first and last names\n",
    " # Normalize marital status: trim spaces, convert to uppercase, replace abbreviations with full words,\n",
    "    # and fill any nulls with \"Unknown\". Operation is scoped only to the 'cst_marital_status' column.\n",
    " # Normalize gender: trim spaces, convert to uppercase, replace abbreviations with full words,\n",
    "    # and fill any nulls with \"Unknown\". Scoped only to 'cst_gndr' column.\n",
    " # Convert create date column from string to DateType (format: yyyy-MM-dd)\n",
    " # Add current timestamp as a new column indicating when data was processed into the DWH\n",
    " # Drop the '_rescued_data' column generated by Autoloader if it exists (contains malformed rows)\n",
    "silver_df=(\n",
    "    df_cust_info\\\n",
    "        .withColumn(\"cst_id\",col(\"cst_id\").cast(\"int\"))\n",
    "        .withColumn(\"cst_key\",col(\"cst_key\"))\n",
    "        .withColumn(\"cst_firstname\", trim(col(\"cst_firstname\")))\n",
    "        .withColumn(\"cst_lastname\",trim(col(\"cst_lastname\")))\n",
    "        .withColumn(\"cst_marital_status\",trim(upper(col(\"cst_marital_status\")))).na.replace(marital_status_map,subset=[\"cst_marital_status\"]).na.fill({\"cst_marital_status\" : \"Unknown\"},subset=[\"cst_marital_status\"])\n",
    "        .withColumn(\"cst_gndr\",trim(upper(col(\"cst_gndr\")))).na.replace(gender_map,subset=[\"cst_gndr\"]).na.fill({\"cst_gndr\" : \"Unknown\"},subset=[\"cst_gndr\"])\n",
    "        .withColumn(\"cst_create_date\",to_date(col(\"cst_create_date\"),\"yyyy-MM-dd\"))\n",
    "        .withColumn(\"DWH_create_date\", current_timestamp())\n",
    "        .drop(col(\"_rescued_data\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Write transformed data to the Silver Delta table:\n",
    "# - Append new records\n",
    "# - Allow schema evolution if new columns appear\n",
    "silver_df.write\\\n",
    "    .format(\"delta\")\\\n",
    "        .option(\"overwriteSchema\", \"true\")\\\n",
    "            .mode(\"append\")\\\n",
    "                .saveAsTable(\"workspace.silver_crm_erp.cust_info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95d925e4-31fe-43ff-80b4-b71c2c059b15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### **LOAD BRONZE PRD_INFO TO SILVER PRD_INFO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9155900-b0df-4d3f-8abb-9b0c18e66b6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read the Bronze Delta table as a batch DataFrame (snapshot of current data)\n",
    "Split 'prd_key' to extract category components.\n",
    "Define product line mapping.\n",
    "Define window spec for ordering by product start date per product key.\n",
    "\"\"\"\n",
    "\n",
    "df_prd_info=spark.read.format(\"delta\").load(\"/Volumes/workspace/bronze_crm_erp/bronze_volume/bronze_crm_data/prd_info/data/\")\n",
    "\n",
    "split_col= split(col(\"prd_key\"),\"-\")\n",
    "\n",
    "product_map ={\n",
    "    \"R\":\"Road\",\n",
    "    \"S\":\"Other Sales\",\n",
    "    \"M\":\"Mountain\",\n",
    "    \"T\":\"Touring\"\n",
    "}\n",
    "\n",
    "window_spec = Window.partitionBy(col(\"prd_key\")).orderBy(col(\"prd_start_dt\"))\n",
    "\n",
    "\"\"\"\n",
    "Transformations for Silver layer:\n",
    "- Cast 'prd_id' to int type\n",
    "- Create 'cat_id' by concatenating first two parts of 'prd_key' separated by '_'\n",
    "- Rebuild 'prd_key' by concatenating all parts from 3rd onward separated by '-'\n",
    "- Keep product name column as is\n",
    "- Cast 'prd_cost' to int and fill nulls with 0\n",
    "- Normalize 'prd_line' by trimming, uppercasing, replacing codes from mapping, and filling nulls with 'Unknown'\n",
    "- Convert 'prd_start_dt' string to DateType\n",
    "- Compute 'prd_end_dt' using lead window function on 'prd_start_dt' per 'prd_key'\n",
    "- Fill null 'prd_end_dt' values with current date\n",
    "- Add current timestamp column 'DWH_update_date' to mark processing time\n",
    "- Drop '_rescued_data' column if exists (for malformed rows)\n",
    "\"\"\"\n",
    "\n",
    "silver_prd_df=(\n",
    "  df_prd_info\\\n",
    "    .withColumn(\"prd_id\",col(\"prd_id\").cast(\"int\"))\n",
    "    .withColumn(\"cat_id\",concat_ws(\"_\",split_col.getItem(0), split_col.getItem(1)))\\\n",
    "    .withColumn(\"prd_key\", concat_ws('-',slice(split_col,3,size(split_col)-2)))\n",
    "    .withColumn(\"prd_nm\",col(\"prd_nm\"))\n",
    "    .withColumn(\"prd_cost\",col(\"prd_cost\").cast(\"int\")).na.fill({\"prd_cost\":\"0\"}, subset=[\"prd_cost\"])\n",
    "    .withColumn(\"prd_line\",trim(upper(col(\"prd_line\")))).na.replace(product_map,subset=[\"prd_line\"]).na.fill({\"prd_line\":\"Unknown\"}, subset=[\"prd_line\"])\n",
    "    .withColumn(\"prd_start_dt\",to_date(col(\"prd_start_dt\"),\"yyyy-MM-dd\"))\n",
    "    .withColumn(\"prd_end_dt\",lead(col(\"prd_start_dt\"), 1).over(window_spec))\n",
    "    .withColumn('prd_end_dt',coalesce(col(\"prd_end_dt\"),current_date()))\n",
    "    .withColumn(\"DWH_update_date\", current_timestamp())\n",
    "    .drop(col(\"_rescued_data\"))\n",
    "\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Reorder columns so that 'cat_id' appears right after 'prd_id'.\n",
    "\"\"\"\n",
    "def reorder_columns(silver_prd_df,insert_col,after_col):\n",
    "  # Remove 'insert_col' if it already exists to avoid duplication\n",
    "  new_silver_prd_df=[c for c in silver_prd_df.columns if c != insert_col]\n",
    "\n",
    "  # Find index of the column after which to insert\n",
    "  indx=new_silver_prd_df.index(\"prd_id\")\n",
    "\n",
    "  # Insert 'insert_col' right after 'after_col'\n",
    "  new_silver_prd_df=new_silver_prd_df[:indx+1]+[\"cat_id\"]+new_silver_prd_df[indx+1:]\n",
    "\n",
    "  # Reorder DataFrame columns accordingly\n",
    "  silver_prd_df =silver_prd_df.select(new_silver_prd_df)\n",
    "  return silver_prd_df\n",
    "\n",
    "silver_prd_df=reorder_columns(silver_prd_df,\"cat_id\",\"prd_id\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Write transformed data to the Silver Delta table:\n",
    "  - Append new records\n",
    "  - Allow schema evolution if new columns appear\n",
    "\"\"\"\n",
    "\n",
    "silver_prd_df.write\\\n",
    "  .format(\"delta\")\\\n",
    "    .option(\"overwriteSchema\",\"true\")\\\n",
    "      .mode(\"append\")\\\n",
    "        .saveAsTable(\"workspace.silver_crm_erp.prd_info\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7352076413357028,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
